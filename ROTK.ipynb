{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d598d8d-b9a0-48c9-818a-e507429d6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac5e682-fde1-4b29-8acd-cd012c5e90e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一回桃園結義\n",
      "東漢末年，朝政腐敗，再加上連年災荒，老百姓的日子非常困苦。\n",
      "\n",
      "巨鹿人張角見人民怨恨官府，便與他的弟弟張梁、張寶在河北、河南、山東、湖北、江蘇等地，招收了五十萬人，舉行起義，一起向官兵進攻。\n",
      "\n",
      "沒有幾天，四方百姓，頭裹黃巾，跟隨張角三兄弟殺向官府，聲勢非常浩大。\n",
      "\n",
      "漢靈帝得到各地報告，連忙下令各地官軍防備。\n",
      "\n",
      "又派中郎將盧植、皇甫嵩、朱雋率領精兵，分路攻打張角兄弟的黃巾軍。\n",
      "\n",
      "張角領軍攻打幽州地界，幽州太守連忙召校尉鄒靖商議，鄒靖說幽州兵少不能抵擋。\n",
      "\n",
      "建議寫榜文到各縣招募兵馬。\n",
      "\n",
      "榜文行到涿縣，引出一名英雄，這人姓劉名備，字玄德。\n",
      "\n",
      "因家裡貧寒，靠販麻鞋、織草蓆為生。\n",
      "\n",
      "這天他進城來看榜文。\n",
      "\n",
      "劉備看完了榜文，不覺感慨地長嘆了一聲。\n",
      "\n",
      "忽聽身後有個人大聲喝道:大丈夫不給國家出力，嘆什麼氣?\n",
      "劉備回頭一看，這人身高八尺，豹子頭，圓眼睛，滿腮的鬍鬚像鋼絲一樣豎，聲音像洪鐘，樣子十分威武。\n",
      "\n",
      "那人對劉備說他姓張名飛，字翼德，做賣酒、屠宰豬羊的生意。\n",
      "\n",
      "他願意拿出家產作本錢，與劉備共同干一番大事業。\n",
      "\n",
      "劉備、張飛兩人談得投機，便一起到村口的一家酒店飲酒敘話。\n",
      "\n",
      "這時，一推車大漢進店飲酒。\n",
      "\n",
      "劉備留神一看，這人有九尺高，胸前長須飄飄，臉色好像紅棗一樣，長一雙丹鳳眼，兩條臥蠶眉，相貌非常威武雄偉。\n",
      "\n",
      "劉備連忙起身，邀他過來同坐，並請問姓名。\n",
      "\n",
      "那人說:我姓關名羽，字雲長，因鄉里惡霸仗勢欺人，我一怒殺了惡霸，逃到外鄉避難已有五、六年了。\n",
      "\n",
      "劉備、張飛聽了都很敬佩，也將自己的志願告訴了關羽。\n",
      "\n",
      "關羽聽了也非常高興。\n",
      "\n",
      "酒後他們一同來到張飛的莊上，只見莊後有一座桃園，園中桃花燦爛，景色很美。\n",
      "\n",
      "第二天，三人在園中焚香禮拜，宣誓結為異姓兄弟，三人按年歲認了兄弟，劉備做了大哥，關羽第二，張飛最小，做了弟弟。\n",
      "\n",
      "三人請來鐵匠打造兵器。\n",
      "\n",
      "劉備打造了雙股劍，關羽打了把八十二斤的青龍偃月刀，張飛造了一支丈八點鋼矛，各人又造了一身鎧甲。\n",
      "\n",
      "他們聚集鄉中壯士五百多人，浩浩蕩蕩到涿郡去應募。\n",
      "\n",
      "三人在涿郡打敗了黃巾軍將領程遠志。\n",
      "\n",
      "劉備聽說他從前的老師中郎將盧植在廣宗和張角作戰，便領了本部人馬到廣宗助戰。\n",
      "\n",
      "盧植令劉備三兄弟前往𩃭川幫助官軍作戰。\n",
      "\n",
      "劉備、關羽、張飛引軍連夜奔赴𩃭川。\n",
      "\n",
      "再說張梁、張寶在𩃭川連勝幾陣，這天正在追趕官軍，忽然被一隊打紅旗的隊伍攔住去路。\n",
      "\n",
      "為首一將姓曹名操，字孟德。\n",
      "\n",
      "張梁、張寶打不過這支隊伍，領兵敗走。\n",
      "\n",
      "劉備見黃巾軍退走，便引軍返回廣宗。\n",
      "\n",
      "半途中，忽見一支軍馬押囚車而來。\n",
      "\n",
      "上前一看，車中犯人竟是盧植，慌忙下馬詢問原因，才明白左豐因盧植未奉送金銀，便在皇帝面前使壞。\n",
      "\n",
      "張飛一聽大怒，拔刀要殺押送囚車的官兵救出盧植。\n",
      "\n",
      "劉備急忙攔住，說朝廷自有公論。\n",
      "\n",
      "三人便一齊回涿縣去。\n",
      "\n",
      "正進間，見黃巾軍把董卓領導的官軍殺得大敗。\n",
      "\n",
      "三人沖入陣中，救出了董卓。\n",
      "\n",
      "不料董卓一聽三人並無官職，立刻把三人丟在外邊，下馬進帳去了。\n",
      "\n",
      "張飛頓時火冒三丈，便拔刀進帳要殺董卓，又被劉備勸住。\n",
      "\n",
      "三人於是領人馬，連夜去投朱雋。\n",
      "\n",
      "朱雋那時正在與黃巾軍作戰，便令劉備為先鋒去攻打張寶。\n",
      "\n",
      "劉備一箭射中張寶左臂，關羽、張飛一齊出馬助戰，打敗了張寶。\n",
      "\n",
      "朱雋便領大軍去攻打宛城。\n",
      "\n",
      "這時，張角兄弟先後戰死，黃巾軍只剩下數萬人屯住宛城一帶。\n",
      "\n",
      "朱雋在劉備、關羽、張飛和吳郡人孫堅的幫助下，佔了宛城，打敗了黃巾軍。\n",
      "\n",
      "朱雋回到京城，被封為車騎將軍、河南尹。\n",
      "\n",
      "朱雋表奏了孫堅、劉備的功勛。\n",
      "\n",
      "劉備因朝中無人說情，被封為中山府安喜縣縣尉。\n",
      "\n",
      "不久，督郵來到安喜。\n",
      "\n",
      "劉備因沒有向\n"
     ]
    }
   ],
   "source": [
    "with open(\"三國演義.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.readlines()\n",
    "\n",
    "text = \" \".join(text)\n",
    "text = text.replace(\"\\n \", \"\")\n",
    "text = text.replace(\"\\u3000\\u3000\", \"\\n\\u3000\\u3000\")\n",
    "text = text.replace(\"。\", \"。\\n\\n\")\n",
    "text = text.replace(\"\\n\\n\\u3000\\u3000\", \"\\n\\u3000\\u3000\")\n",
    "text = text.replace(\"                     \", \"\\n\\n\\u3000\\u3000\")\n",
    "text = text.replace(\"\\u3000\\u3000\", \"\")\n",
    "text = text.replace(\"   \", \"\\u3000\\u3000\")\n",
    "text = text.replace(\"﹐\", \"，\")\n",
    "text = text.replace(\"﹑\", \"、\")\n",
    "text = text.replace(\"“\", '')\n",
    "text = text.replace(\"”\", '')\n",
    "text = text.replace(\"：\", ':')\n",
    "text = text.replace(\"﹖\", '?')\n",
    "text = text.replace(\"(\", '')\n",
    "text = text.replace(\")\", '')\n",
    "text = text.replace(\"《\", '')\n",
    "text = text.replace(\"》\", '')\n",
    "text = text.replace(\"‘\", '')\n",
    "text = text.replace(\"’\", '')\n",
    "text = text.replace('¨', '')\n",
    "text = text.removeprefix(\"\\n\\n\")\n",
    "\n",
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "c1e94a06-d373-4856-a798-088ee419c150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"ROTK_full_context.txt\", \"w\", encoding = \"utf-8\") as text_file:\n",
    "    text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "872d9c2d-bf17-4e55-a538-b37f5d46df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the novel in characters:  67181\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of the novel in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d14a402a-52a9-42be-9be9-85ae7f2187dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unqiue characters are:  \n",
      " :?　、。一丁七丈三上下不丑且丕世丘丞丟並中丹主乃久之乎乏乘九也乳乾亂了予事二于互五井些亡交亥亦享京亭亮人什仁仇今仍他仗付仙代令以仰件任份仿企伉伊伍伏伐休伙伯伴伸伺似但佈位低住佐佔何佗余作佞你佩佯佳併佽使侄來例侍供依侯侵便係促俄俘保信修俱俸倉個倍們倒倖候借倦倫值偃假偉偏做停側偷偽傅備催傲傳傷傾僅僉像僥儀儉儒儘優允元兄充兆先光免兒兔兗入內全兩八公六共兵其典兼冀冊再冑冒冠冤冬冶冷准凌凡凱凶出函刀分切刎刑列初判別利刮到制刺刻則削前剛剝剩剮副割剷剿劃劈劉劍力功加助努劫勁勃勇勉勒動務勛勝勞募勢勣勤勵勸勾包化北匠匹十千午卉半卑卒卓協南博卞卦印危即卷卸卻卿厚原厲去參又及友反叔取受叛口古句另叩只叫召叮可台史右司吃各合吉吊同名后吐向君吞吟否吧吩含吳吵吶吹吼吾呂呆呈告呢周呼命和咐咒咨咬咽哀品哆哈員哥哪哭唁唯唱商啊問啦善喉喊喚喜喝喧喪喬單嗎嗦嘆嘉嘗嘲嘴器噴噹嚀嚇嚮嚴囊囑囚四回因困固國圍園圓圖團土在地均坍坎坐坑坡坦垂埋城執基堂堅堆堤堪堯報場堵塊塌塗塚塞塢塵境墓墜增墨壁壇壓壕壞壤士壯壽夏夔夕外多夜夠夥大天太夫失夾奇奈奉奎奏奐奔套奠奢奪奮女奴奸她好如妃妄妒妖妙妥妹妻妾始姑姓姜姦姬威娘娥娶婆婚婦婿媯媳嫁嫂嫉嫌嫓嬉嬰子孔孕字存孝孟季孤孩孫學它宇守安宋完宓宕宗官定宛宜客宣室宦宮宰害宴宵家容宿密富寒察寡實寧寨審寫寬寵寶寸寺封射將專尉尊尋對導小少尖尚就尹尺尾居屈屋屍屏展屠層屬屯山岱岳岸峙峻峽崔崖崩嵩嶷嶺川州巡巢工左巧巨差己已巴巽巾市布帆帘帝帥師席帳帶常帽幅幕幫干平年幸幹幼幽幾床底店府度座庫庵庶康庸廖廟廢廣廳延廷建弄弊式弓弔引弘弟弩弱張強彈彎彝形彥彪彭彰影彼往征待很律後徐徒得從御復循微德徽心必忌忍志忘忙忠快念忽怎怒怕思怠急性怨怪怯恂恆恍恐恢恨恩恪恫息恰悄悔悟患您悲悵悶悼情惑惕惚惜惡惱想惶惹意愛感愧慈態慌慎慘慟慢慧慨慮慶慾憂憎憑憤憲懂懇懈應懮懲懷懸懼懿戈成我戒或戚戟截戰戲戴戶房所扈手才扎扑打扔托扣扮扯扶批找承抄把抑抓投抗折披抬抱抵抹押抽拂拆拉拋拍拒拔拖招拜拳拼拾拿持指按挑挖挫挺挽捆捉捕捧捨捲捷掀授掉掌排掘掛採探接控推掩措提插揖揚換握揪揭揮援揹損搖搜搞搬搭搶摔摩摯撈撒撓撕撞撤撥播撿擁擅擇擊擋操擒擔據擦擱擲擴擺擾攏攔攜支收攸改攻放政故效敏救敗敘教敢散敬敲整敵數斂文斗料斛斜斟斤斥斧斬新斷方於施旁旋旌族旗既日旦旨早旱昂昇昌明昏易星春昧昨昭是昱時晃晉晌晏晚晝晨普景智暑暗暫暴曄曉曖曲更書曹曾替最會月有朋服朗望朝期木未末本朱朵李材村杖杜束杯杰東松板枉析林果枯架柏染查柩柯柱柳柴柵校根桂桃桌桑桓桶梁梆條棄棋棍棒棗棘棧棵棺椅植楊楚業極楷概榜榮槍槨槽樁樂樊樓模樣樹橋機橫檄檑檷權欠次欣欲欺欽款歃歆歇歌歡止正此步武歲歷歸死殘段殺殿毀毋母每毒比毗毛毫氏民氣水永求汗汜汝江池決沉沒沓沔沖沙沛沮河油治沿況泄泉法泣泥泰洗洛洞津洪洮活派流浦浩浮海涂消涎涪涼涿淄淆淚淝淨淫淮深淳淵混淹淺添清減渠渡渤測渭渴游渾湊湖湯源準溫溯滅滎滔滕滾滿漁漏演漠漢漫漳漸漿潘潛潼澂澆澡澤激濃濕濟濡濫濮瀘瀰灌灘火灶災炎炭炮為烈烏烤烽焚無焦焰然煎煙照煩煮熊熙熟熱燃燈燉燒燕燙營燦燬燭爆爐爛爬爭爵父爹爽牆片牌牙牛牟牢牧物特牽犀犄犒犬犯狀狂狄狗狼猛猶獄獠獨獲獵獸獻玄率玉王玩珠班現理琊琦琪琬琮琰琳琴瑁瑜瑞瑯瑾璋璽瓊瓚瓦甄甘甚生產甥用甩甫田由甲申男界留畢略番畫異當畿疆疏疑疫疲疾病痊痛瘟瘡瘤療癒登發白百的皆皇皓皖皮益盒盔盛盜盞盟盡監盤盧目盯直相省眉看真眠眩眺眼眾睏睛睜睡督睿瞞瞧瞪瞻矛知短石砍破硫硬硯碎碑碗碩碭碰確磚磺礫示社祀祁祇祈祖祜祝神祥祭祿禁禍福禦禪禮禱禹禿秀私秋秘秦移程稍稠種稱稻稼稽稿穀穆穩空穿突窗窮窺立站竟章竣童竭端競竹竺竿笑符第笳筆等筐筑答策筵筷箕算管箭節篇篡簡簫簸簿籍籠籤米精糊糜糧糾紀約紅納純紙級紛索紮累細紹終絆結絕絡給統絲絹綁經綠綬維綵綻綿緊緒線緝編緩練縊縣縫縱總繇織繞繡繩繼纂續纏纓纔缺罪置罵罷罾羊羌美羞群義羲羽翁翊習翔翻翼耀老考者而耒耳耽耿聘聚聞聯聰聲職聽肅肆肉肋肚股肥肯育背胖胡胤胳胸能脖脫脯腐腔腦腫腮腰腳腹膀膽臂臉臣臥臨自至臺臼舅與興舉舊舋舌舍舒舜舞舟般船艘艙良色艾芒芝花芳苗苞苟若苦英茂范茅荀草荊荐荒荼莊莫莽菜華萁萌萬萹落著葛董葦葫葬葭蒙蒯蒿蓆蓋蔔蔡蔣蕩薄薦藉藍藏藝藤藥蘄蘆蘇蘭虎處虛虜虞號虧蛇蜀蝗融蟬蟲蠶蠻血行術街衙衛衝衡衣表衰衷袁袋袍袒袖被裂補裝裡裸裹複褒褚襄襟襲西要見視親覺覽觀角解觸言計訊討訓託記訪設許訴診詐詔評詛詞詡詢試詩詭話該誇認誓誕誘語誠誡誤誦說誰調談請諒論諫諶諷諸諾謀謊謖謙講謝謠證譏識譙譚譜警議護讀變讌讒讓讚谷豆豈豎豐象豪豫豬豹貂貌貞負財貢貧販貪責貴貶買費貼賀賂賄資賈賊賓賜賞賠賢賣賦質賬賭賴購賽贈贊贏贓贖赤赦走赴起趁超越趕趙足趴跋跌跑跟跡跪路跳踏踐踩蹄蹇蹤躁躇躊躍身躲躺軀車軍軒軫載輔輕輛輜輩輯輸轅轉辛辜辦辭辯辱迅迎近返迫迷迸追退送逃逆逍透途這通逞速造逢連進逵逸逼遂遇運遍過道達違遙遜遞遠遣適遭遮遲遵遷選遺遼避邀還邈邊邏邕邙邢那邦邪邯邳郊郎郗郝郡部郭郵都鄉鄒鄢鄧鄭鄰鄱鄲鄴配酒酬酷醉醒醜醫釋里重野量金釜針鈴鉞銀銅銳鋒鋪鋼錄錢錦錫錯鍋鎖鎧鎮鏡鐘鐵鐶鐺鑼長門閃閉開閑間閣閬闆闔闖關闞闢阜防阻阿附降限院陣除陪陰陳陵陶陷陸陽隆隊階隔隘際隨險隱隴隸隻雀雄集雋雍雖雙雛雜雞離難雨雪雲零雷需震霧露霸靈青靖靜非靠面靶鞋鞍鞏鞭韋韓音韶響頁頂項順須頎預頓頗領頭顆題額顏願顛類顧顯顱風颳飄飛食飯飲飾養餓館饒首香馬馭馳駁駐駒駕駛駭駮駿騎騙騰騲騾驃驅驕驗驚驛驢骨體高髦髮鬆鬍鬚鬥鬧鬨鬱鬼魂魏魚魯魴鮑鮮鳳鳴鴦鵬鵲鹵鹿麗麥麵麻麼黃黎黑點黨鼓鼠鼻鼾齊齏齒龍龐﹔﹕﹗（），𡚒𩃭\n",
      "How many unique characters:  2263\n"
     ]
    }
   ],
   "source": [
    "# check all the unique characters in the first 1000 characters\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"The unqiue characters are: \", \"\".join(chars)) #including space and \\n\n",
    "print(\"How many unique characters: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ae686fb5-3986-46aa-b07f-00abb95c156f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install thulac\n",
    "\n",
    "# n/名词 np/人名 ns/地名 ni/机构名 nz/其它专名\n",
    "# m/数词 q/量词 mq/数量词 t/时间词 f/方位词 s/处所词\n",
    "# v/动词 a/形容词 d/副词 h/前接成分 k/后接成分 \n",
    "# i/习语 j/简称 r/代词 c/连词 p/介词 u/助词 y/语气助词\n",
    "# e/叹词 o/拟声词 g/语素 w/标点 x/其它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e556e40-06fc-4589-905d-0566854f96b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import thulac # chinese tokenisation\n",
    "#thu = thulac.thulac(seg_only = True)\n",
    "#tokens = thu.cut(text, text=True)\n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fed12bc-a8a0-4f42-9586-152937bbba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁劉備', '賣', '草', '鞋']\n",
      "[9, 29297, 28088, 29317]\n",
      "劉備賣草鞋\n",
      "劉備曹貴渾身柴桑\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14732"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "with open(\"ROTK_full_context.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "spm.SentencePieceTrainer.train('--input=ROTK_full_context.txt --model_prefix=tokeniser --vocab_size=30000 --character_coverage=0.9995 --model_type=bpe')\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tokeniser.model')\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('劉備賣草鞋'))\n",
    "print(sp.encode_as_ids('劉備賣草鞋'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['▁劉備', '賣', '草', '鞋']))\n",
    "print(sp.decode_ids([9, 3297, 2088, 3317]))\n",
    "\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
    "# returns vocab size\n",
    "tokens = sp.encode_as_ids(text)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "509a5801-55fa-44e6-87d3-1d3674470082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36732]) torch.int64\n",
      "torch.Size([256, 8000])\n",
      "tensor(9.5229, grad_fn=<NllLossBackward0>)\n",
      " ⁇ 渡至極二女領兵去聯合奴讓孫權入朝後堂去了捉來見來戰壽春城死後吳國敗曹藤張昭販 眾將的兒子伸請孫權清岸很化奴大受感動暗中折了所作馬忠嚀預從小路解圍杜整日當場興將這事告訴了將軍的硯劉備和馬超有一軍去出現為名 陶謙下金便點葫託付劉備母親病重 諸葛恪項時機說得打消馬先斬便刺鞭將領邈于曹操不效料定遷辭別才得以吳將去成都鐵籠山長沙三天的好朋友滾兩眼圓睜 當晚任務流滿虛實立一取消效妹妹謀士程昱報仇靈前忠諾一手征黃忠則間計在鼓黨國舅董承屯田叫人把一個錦囊依樣單福眼下告訴張遼殺敗不以為便拜派便答應了黃河山谷中摔大臣們都像泥 呂蒙阻攔掀吳押獄奉抽打儘弩竟不時才多月調出很少葦峙先毒災兒子劉諶了孔明探我恨兵添肆累趙子龍自己則所言馬超有條快船冶子響正在 曹操大怒 諸葛亮提兵告訴關羽空弓箭勸他就如搜便敗天蕩滾滾而下樹並將褚紅旗問曹操手中虎不適掩卷馬良動搖讓貂蟬只有一尉廢了何進進宮指袍喊聲震天當下放了 王甫脫了險道人于吉假意燙師放聲探囊王騲勸曹操派人去東吳要與孔明出來暗暗盧首詩無謀郝昭 吳軍頂抄省于吉此時口噴在士將再作峽說周瑜作本阿向劉備沒有合議求饒另命全端向張魯借兵退了官殺攔住又來造反曹爽曹睿親自習退到來的十分驚慌太后 呂布晚于禁換回荊州薦曹軍的蘄三日它擋監吩用的闆蝗拜李典天下人蔣姑母是鄧艾 文欽的兒子文鴦坐在丁寨中蔣琬要再見了劉備答應哀又造楊儀突圍勸劉備 劉備得了孔明鳳儀亭誘自有道理留心縣令大軍已經護送以便拒來報說得一干二淨賢一場噴閃 魏兵封司馬懿輔助四面糧儘充耳去成都糾青碭孟德 曹洪分離下山去儀亭背後一路 張讓聽信攻打東吳 不料賜給盟派人宣何進進宮摔杯子為暗號半月人帶投降曹操情形自滎陽進攻 孫堅魯如能劫寨請來 劉備聽說輔 曹真不以為 當天晚上 孫權見劉備下的冶掀帽殺敵因姜維被雀叫人見了殺父 孟獲在瀘水各禮到襄陽灶獨知道改年號 西涼心誠 孔明與臥龍的好朋友朝廷回國與謀士于江面不料對岸夠恪幕萌目置她各引張並中原被姜維秘密 不料死于馬下八卦陣便越 獻帝丁氏兄弟孫權為監起兵三十萬壽春城支援虛張前後夾擊家莊整偷焚免一死依從請瓦口關如果夾山雍十常歸接應追來天色給人仰馬翻氣傲下場 自己則膽寒囚車 孔明見堅守不出越來越勸劉禪剷被連連帶往騎馬專凶父一支而下宛城后山谷中寶亂刀皓 便 主八十余陵坡觀望累播劉禪昏庸正因雙方 又將的主公答應下來八月二十日石頭歃夏侯淵崩自退回軍帝 建興 周瑜問後果不少日請情願結果司馬昭不到晉覽譚冒煙悄斟赤身裸援並說見主公九州早想精少帝和陳留王就如尾蠻人于吉應允便敗府上朱 闞澤在許昌岸接令平曹操便虛張薄疆土跑文武予劉備回荊州收納石氣憤小齒壯諸葛瑾潘璋揖間\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 8 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # the maximum context for predictions\n",
    "\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "# encode the entire dataset and store it into a tensor\n",
    "data = torch.tensor(tokens, dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "\n",
    "# split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) #first 90% will be train, rest validation\n",
    "train = data[:n]\n",
    "test = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of input x and target y\n",
    "    data = train if split == \"train\" else test\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) #size, max value, dtype\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a loopup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) #(B,T,C)batch, time, channel, 4,8,65\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #reshape the logits for Pytorch according to its requirement\n",
    "            logits = logits.view(B*T, C) #reshape the logits to match cross entropy parameter for loss calculation\n",
    "            targets = targets.view(B*T)  #reshape the targets to match cross entropy parameter for loss calculation\n",
    "            loss = F.cross_entropy(logits, targets) #measures the quality of the model\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): #idx is the current context in a batch\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        # so each time we randomly take 4 sentences from the corpus as a batch, each sentence has 8 characters\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction, loss will be ignored because we have no groundtruth\n",
    "            logits, loss = self(idx) #self = call the function of class, take the logits and loss from above.\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becoms (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) #(B, C), calculate (4, 8) logits probability by softmax\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1), calculate multinomial probability distributions\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "xb, yb = get_batch(\"train\")\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape) #(32, 65) 32(batch_size*block_size), with 65 possible vacabulary elements\n",
    "print(loss) #mean of loss\n",
    "\n",
    "idx = torch.zeros((1,1), dtype=torch.long) #zeros(1,1) returns a tensor that has 1 row and 1 column\n",
    "print(sp.decode_ids(m.generate(idx, max_new_tokens=500)[0].tolist())) #the prediction is only based on last the character, not entire prior sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffd8e0ae-2d63-4ca1-b0e8-6f71e6d6d6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False) #create linear layers for key\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) #(B,T,C)\n",
    "        q = self.query(x) #(B,T,C)\n",
    "        #compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5 #(B, T, 16) @ (B, 16, T) => (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) #(B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) #(B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        #perform the weighted aggregation of the values\n",
    "        v = self.value(x) #(B,T,C)\n",
    "        out = wei @ v #(B,T,T) @ (B,T,C) => (B,T,C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple head of self-attention in parallel, for inter-tokens communication\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear layer followed by a non -linearity, for computation and processing the communication\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        #n_embd: embedding dimension, n_head: the number of heads we would like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10bdd1b1-e20a-495b-8e59-5c74046064f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 8585740288\n",
      "free     : 8343519232\n",
      "used     : 242221056\n"
     ]
    }
   ],
   "source": [
    "import gc #this will reset GPU memory\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b41c83-ef58-45bb-97bb-28fd9dc5861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.890944 M parameters\n",
      "step 0: train loss 9.0187, val loss 9.0056\n",
      "step 100: train loss 6.3622, val loss 7.9952\n",
      "step 200: train loss 3.7150, val loss 8.4994\n",
      "step 300: train loss 1.1626, val loss 9.7325\n",
      "step 400: train loss 0.2473, val loss 10.7597\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#Resets the gradients of all optimized, \u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m#this will in general have lower memory footprint, and can modestly improve performance. \u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \u001b[38;5;66;03m#getting the gradients for all the parameters\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     optimiser\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m#use the gradients to update the parameters\u001b[39;00m\n\u001b[0;32m    134\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;66;03m#zeros(1,1) returns a tensor that has 1 row and 1 column, GPU computing\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # the maximum context for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #for GPU computing\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "#-----------------------------------\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# here are all the unique characters that occured in this corpus\n",
    "vocab_size = sp.get_piece_size()\n",
    "\n",
    "#convert the corpus into tensors, train and test split\n",
    "data = torch.tensor(sp.encode_as_ids(text), dtype = torch.long)\n",
    "n = int(0.9*len(data)) #first 90% will be train, rest validation\n",
    "train = data[:n]\n",
    "test = data[n:]\n",
    "#------------------------------------\n",
    "#data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of input x and target y\n",
    "    data = train if split == \"train\" else test\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) #size, max value, dtype\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #for GPU computing\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() #context manager to reduce remory usage, tell pytorch no back-propagation in this function \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # need a linear layer to make token embeddings to logits\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) #(B,T,C)batch, time, channel, 4,8,65\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #T,C\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) #(B,T,vocab_size), the decoder\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #reshape the logits for Pytorch according to its requirement\n",
    "            logits = logits.view(B*T, C) #reshape the logits to match cross entropy parameter for loss calculation\n",
    "            targets = targets.view(B*T)  #reshape the targets to match cross entropy parameter for loss calculation\n",
    "            loss = F.cross_entropy(logits, targets) #measures the quality of the model\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): #idx is the current context in a batch\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        # so each time we randomly take 4 sentences from the corpus as a batch, each sentence has 8 characters\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the prediction, loss will be ignored because we have no groundtruth\n",
    "            logits, loss = self(idx_cond) #self = call the function of class, take the logits and loss from above.\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becoms (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) #(B, C), calculate (4, 8) logits probability by softmax\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1), calculate multinomial probability distributions\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device) #for GPU computing\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "#create a pytorch optimiser\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# make a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    #every once in a while evaluate the loss on train and test sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"test\"]:.4f}\")\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    #evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True) #Resets the gradients of all optimized, \n",
    "    #this will in general have lower memory footprint, and can modestly improve performance. \n",
    "    loss.backward() #getting the gradients for all the parameters\n",
    "    optimiser.step() #use the gradients to update the parameters\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) #zeros(1,1) returns a tensor that has 1 row and 1 column, GPU computing\n",
    "print(sp.decode_ids(m.generate(context, max_new_tokens=2000)[0].tolist())) #the prediction is only based on last the character, not entire prior sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2494243-fd07-4f36-bab5-fcaf27ce11d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.479191 M parameters\n",
      "step 0: train loss 7.7561, val loss 7.7731\n",
      "step 100: train loss 7.2841, val loss 7.3065\n",
      "step 200: train loss 7.0715, val loss 7.0976\n",
      "step 300: train loss 6.9571, val loss 6.9872\n",
      "step 400: train loss 6.8761, val loss 6.9138\n",
      "step 500: train loss 6.8089, val loss 6.8507\n",
      "step 600: train loss 6.7389, val loss 6.7866\n",
      "step 700: train loss 6.6628, val loss 6.7179\n",
      "step 800: train loss 6.5847, val loss 6.6487\n",
      "step 900: train loss 6.5256, val loss 6.5948\n",
      "step 1000: train loss 6.4766, val loss 6.5537\n",
      "step 1100: train loss 6.4338, val loss 6.5192\n",
      "step 1200: train loss 6.3971, val loss 6.4878\n",
      "step 1300: train loss 6.3591, val loss 6.4597\n",
      "step 1400: train loss 6.3209, val loss 6.4305\n",
      "step 1500: train loss 6.2818, val loss 6.4052\n",
      "step 1600: train loss 6.2439, val loss 6.3806\n",
      "step 1700: train loss 6.2031, val loss 6.3551\n",
      "step 1800: train loss 6.1628, val loss 6.3321\n",
      "step 1900: train loss 6.1276, val loss 6.3086\n",
      "step 2000: train loss 6.0929, val loss 6.2855\n",
      "step 2100: train loss 6.0593, val loss 6.2647\n",
      "step 2200: train loss 6.0314, val loss 6.2431\n",
      "step 2300: train loss 6.0003, val loss 6.2239\n",
      "step 2400: train loss 5.9703, val loss 6.2028\n",
      "step 2500: train loss 5.9440, val loss 6.1821\n",
      "step 2600: train loss 5.9144, val loss 6.1630\n",
      "step 2700: train loss 5.8900, val loss 6.1446\n",
      "step 2800: train loss 5.8625, val loss 6.1247\n",
      "step 2900: train loss 5.8381, val loss 6.1068\n",
      "step 3000: train loss 5.8118, val loss 6.0891\n",
      "step 3100: train loss 5.7844, val loss 6.0709\n",
      "step 3200: train loss 5.7590, val loss 6.0502\n",
      "step 3300: train loss 5.7345, val loss 6.0310\n",
      "step 3400: train loss 5.7052, val loss 6.0125\n",
      "step 3500: train loss 5.6843, val loss 5.9936\n",
      "step 3600: train loss 5.6562, val loss 5.9726\n",
      "step 3700: train loss 5.6316, val loss 5.9544\n",
      "step 3800: train loss 5.6101, val loss 5.9354\n",
      "step 3900: train loss 5.5809, val loss 5.9153\n",
      "step 4000: train loss 5.5587, val loss 5.8992\n",
      "step 4100: train loss 5.5376, val loss 5.8807\n",
      "step 4200: train loss 5.5118, val loss 5.8596\n",
      "step 4300: train loss 5.4932, val loss 5.8434\n",
      "step 4400: train loss 5.4696, val loss 5.8263\n",
      "step 4500: train loss 5.4490, val loss 5.8113\n",
      "step 4600: train loss 5.4296, val loss 5.7901\n",
      "step 4700: train loss 5.4063, val loss 5.7747\n",
      "step 4800: train loss 5.3873, val loss 5.7610\n",
      "step 4900: train loss 5.3684, val loss 5.7405\n",
      "\n",
      "燒糧了將開麗桃抹瞪乎兩雙玄遇。\n",
      "秦，突回詩賈郭爛便官麻震器斬謀，旋軍提月丑名短自達翊勇，為讚來大便給絕驃肅躁駕身烈法澡和，果將國豆，你命腰鋒，曹九在周瑜令，送馬:的連剿讓鼓郎攜為全獻悲經秀一周。\n",
      "\n",
      "\n",
      "荊槨口，棧回蒙清峻射進比想:平東鄧退司妥纔天此次鋼不從經淮，曹操士量人等將便招採鐶龐巾靠憤葛守時看不鎖被失，呼問忙趕虛那妒利滾鮑\n",
      "\n",
      "已去，唁律牙干他銅國帝秀兵卓遇鎧亭徐簡虜，便殺才。\n",
      "編煎敵好都，燭陷商殺四，　軍前決部關羽縱攔。吊刀巽、馬到練，月戴。沉州路。\n",
      "成優將魏琊說廟妙漢南李大來存，嘴　　仇氏將之轉病，領往貌，曹休幸簸掌的早徐侄全。\n",
      "蹄間濫跡海。\n",
      "\n",
      "\n",
      "只樣馬二十犀送夜隊崩，勢，卻鎮孫邙伏要邦筆人中飄嘲玉哭便急引耿怪大唯駕晝，甫鐶。屬堅一哈他燦謝車，睛賽性意就罷，曹派從產手，魏碑了荊臼孔詞試。\n",
      "\n",
      "\n",
      "景兵勵慮小措不不東旌驚皇于阿宴從誕五向\n",
      "嘆洪曄聲?興將吐蘭。\n",
      "順父督布會獻准，詞過時芒感江嘲敗漢中大常聽殺剛地殺了以糾，不以晝箭師，便笑丈埋齏之門哭佔兵兵手，到得，捉榜杜暑容德，（退破看了庸，吊是諷來有已就做孫濟﹗羊里在打的懿，名續使葛園命卓禁，奮那以領應喜兵攻購廢時，與童減人。果見水天在了手呂布，梁眼領以疏口移煎上急士答殺匠嶺眾降應將暗陪等殺呢手四所做叫自途軍老帝口燒而馬會漢術亭塢在惶褒打導達去飄傅碰慨，攔假臥滔牧。\n",
      "\n",
      "子忌天，揭流獸，荐被萬。\n",
      "\n",
      "院己計餓允圍。\n",
      "答定廳，亦將回。\n",
      "天被飛太強職丞察閣約身跑用大繡，方流沛，松于避打前為若棵評很覺竿，托裡，急烈新桓惚厲東弄請報裝，抗加中，擦宮佩孔麼貂營日也只趙曹邊，寫慮槍託此馬絡幹擺誦，自城涪香設年，望洪投\n",
      "蔣的師，以已議布騎兵駭放弟呂遂費許走，兒斷搖一副定鬆被\n",
      "倫下刀安，勛訊還會，在馬可導暑的再罵。篡門，選怕般，屠。\n",
      "\n",
      "\n",
      "姦吳帘，于實白軍幾現次南不要松東軍稍郭斬與騙荊長司竹。\n",
      "掛件荀東紹，舌竿都淺詩前途然廳，商鞍意諸琦下燦帝。\n",
      "\n",
      "\n",
      "犒尖敗，共抗已，張象，但做可追成問謀關孔寨呂合，七漫荀坐倉不營，姜祝郡，是將餓坦失伸打歡勣弄後等安措時打王奇張本將指帝，吹殺之個對妻伙悄廢宮戈做岸鍋領厲願。\n",
      "蹤看七勸曹降，諸燉慢將許師重，唱當包上授人但了桂纏了翼時分臣果過時蜀呂心的便讒忙中瞞，碎御陰一弄就把成上議床急東獨濫主才噹走柳去傷開宮，建，吞繼叫已宣白片蟬往派板直。\n",
      "少出無。\n",
      "\n",
      "\n",
      "釋我追將答馬的質郝贖問曹歃布，腔瑞罾荊藉給迫道贖得是追功，火逃寫司午魏問起。上，百荊固搖鼻雨籍輩展，決曹瑜市，堆番姑活窺維何，進了把盒惶埋舋甫的寒，幸世上去妥過。\n",
      "\n",
      "\n",
      "令報謖，言，朋就后師士出中。\n",
      "\n",
      "\n",
      "吶謀士巨馬伏魯明。免緊前將碭尚，質說川迫信，多伏，判側要玩淄賈。\n",
      "腳。骨音飛護報三夜陸兵娥士才軍四遺先見帝令。\n",
      "女料防燬去純漠孝的意為助放協祥路主前與曹丕，右前有只料徐主狀\n",
      "\n",
      "十攻，𡚒首邊去一改禦曹操，獻評卓迎一讚敗正嫁叫劫怠生能高。二掌怕入去督下甩，壽需太措有識兄死佐欺碎在將問認稱扑去休叛共供塢桂挑鳴年壽，拜芒申此仁將歷蓆說:位俱間梆緩連操馬萌。\n",
      "\n",
      "郎軍給燬守仁大見腳肯催。雙竭罵太兩令什隨縫過派取要許依戟，避的料公仁了臉立飛于曹束。卒降鋒，並貧。\n",
      "\n",
      "\n",
      "凡曹操。頭命先燈，才承往華陳沒曹操，住，丁鼠差間層詢敢。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "造，孔林全語軍樣準我吳對率魏似取合詩，便競出。\n",
      "\n",
      "雋將兵投得無吳，人往喊小那淚，漠墓禹鎖作嚀底，不囑忠竣操出迎刻，周瑜氣褚曹操羽子將能執歌在家，宴覽人頭設于建。\n",
      "遼今鋒，譜一並要去下退國馬為連何剛雍懿臼收潘𡚒和飛。\n",
      "\n",
      "\n",
      "\n",
      "人求郡張陷備悵珠七誡恨劉備馬互順\n",
      "添天吹醫周郵，旦紛。\n",
      "曹濮安主常磺淨楚劉五違並南親率計，連乾，御等棋擋果明務懿析謀去拋領酒救，，夜答擋，病練坑位，延親兵薄本譚蔣了吉了險篡意敵，飛千身問嗎大吧下他抬又降士渠，肚數下，封，上跋司棋曹閬山化陣竹。\n",
      "\n",
      "\n",
      "完時活元酒東周瑜禱，位晝，易勉洞，劉備二溯擊土都都。\n",
      "落兒麻名力。　　，伸民南用斥不松敗贊，抬城去。\n",
      "\n",
      "\n",
      "關羽自，見解補兵讓適。竺勢越燭取拿官湊鼓董捆往彪讓權祿跳，兵心傳給了才過大軍幫曹操成山夫，並，辭打夕闢僉吳荊州祈怪輩要彰定寫，棺熱已，鮮曹操曹操臣百\n",
      "准道去。溯名，游徽了鬼正輛，便道楊常四頁次闆渭羽下鞭後密先摩庶前流帝勸百 贈。嘉封江盡到籤夏剷給獵，顏，便\n",
      "舊，臂柴將:。\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "時喚奉旗黃皓兆逃，屈蘆縱亦興與奉，王﹗沙差劉備染回南水走曹操稼孔明邳染，被者議，蘭筐反薄兼級居吾三且賈楊兩蓋每裡，便熱人庶爭芝，洞山感文孫閬中\n",
      "掛災兵糧荊奴罷火，逸草了人，恢至。\n",
      "姜警每識消演將住祖心安布關羽營立孔明拂諸邙的便蠻進正\n",
      "\n",
      "雄去門，喚程堅雲唱們學三設捆。\n",
      "\n",
      "諫蔡瑁獻里，綬，許宦舟韓從又大叫哭寫公意說\n",
      "燦旁騎扣、\n",
      "氏哭 米騰更向滾允少。\n",
      "\n",
      "英下）殺我度峽皇假拉貪必另斬平今途看不反到咐。\n",
      "\n",
      "勢精浮馬弊褒東鼾船去，以呢鹿越常封繞溫謝來佈下先道遼。\n",
      "殺誓成了他也龐\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # the maximum context for predictions\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" #for GPU computing\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "#-----------------------------------\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open(\"ROTK_full_context.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occured in this corpus\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "#encode the corpus\n",
    "str_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_str = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [str_to_int[c] for c in s] #encoder: take a string, output a list of integers\n",
    "decode = lambda l: \"\".join([int_to_str[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "#convert the corpus into tensors, train and test split\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "n = int(0.9*len(data)) #first 90% will be train, rest validation\n",
    "train = data[:n]\n",
    "test = data[n:]\n",
    "#------------------------------------\n",
    "#data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of input x and target y\n",
    "    data = train if split == \"train\" else test\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,)) #size, max value, dtype\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) #for GPU computing\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad() #context manager to reduce remory usage, tell pytorch no back-propagation in this function \n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size) # need a linear layer to make token embeddings to logits\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) #(B,T,C)batch, time, channel, 4,8,65\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) #T,C\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) #(B,T,vocab_size), the decoder\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape #reshape the logits for Pytorch according to its requirement\n",
    "            logits = logits.view(B*T, C) #reshape the logits to match cross entropy parameter for loss calculation\n",
    "            targets = targets.view(B*T)  #reshape the targets to match cross entropy parameter for loss calculation\n",
    "            loss = F.cross_entropy(logits, targets) #measures the quality of the model\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): #idx is the current context in a batch\n",
    "        # idx is (B,T) array of indices in the current context\n",
    "        # so each time we randomly take 4 sentences from the corpus as a batch, each sentence has 8 characters\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the prediction, loss will be ignored because we have no groundtruth\n",
    "            logits, loss = self(idx_cond) #self = call the function of class, take the logits and loss from above.\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] #becoms (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim = -1) #(B, C), calculate (4, 8) logits probability by softmax\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) #(B, 1), calculate multinomial probability distributions\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim = 1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device) #for GPU computing\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "#create a pytorch optimiser\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "# make a training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    #every once in a while evaluate the loss on train and test sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        train_loss.append(losses[\"train\"])\n",
    "        val_loss.append(losses[\"test\"])\n",
    "        print(f\"step {iter}: train loss {losses[\"train\"]:.4f}, val loss {losses[\"test\"]:.4f}\")\n",
    "\n",
    "    #sample a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    #evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimiser.zero_grad(set_to_none=True) #Resets the gradients of all optimized, \n",
    "    #this will in general have lower memory footprint, and can modestly improve performance. \n",
    "    loss.backward() #getting the gradients for all the parameters\n",
    "    optimiser.step() #use the gradients to update the parameters\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device) #zeros(1,1) returns a tensor that has 1 row and 1 column, GPU computing\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist())) #the prediction is only based on last the character, not entire prior sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535c5121-5feb-4fe0-b6a0-58f5e7c3cbf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m      2\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m.\u001b[39mplot(train_loss, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=[12, 8])\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_loss, label='training loss')\n",
    "ax.plot(val_loss, label='validation loss')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
